# Literature Review {#sec-litreview}

The following chapter outlines literature relating to the geographic concept of 'place' and its connection with natural language. A renewed interest in the quantitative analysis of place-based concepts has been driven by the large volume of data produced through social media platforms, which is hypothesised to more closely align with place-based geographies compared with traditional data sources. However, few works have considered the textual component of social media platforms, and its ability to generate informal vernacular geographic knowledge. Geographic analysis of social media networks typically focusses on explicit geographic markers which exist as geotags, and consider the associated text to be a by-product of communication between users, with little geographic value. Instead, this chapter outlines that natural language text from these social media instead presents the opportunity to capture a much greater volume of place-based geographic knowledge, which has not been previously considered in detail.

## Introduction

Unlike structured knowledge regarding space, which is dictated by formal administrative definitions, with strict coordinate systems and boundaries, natural language instead contributes knowledge regarding place. Informal communication on social media platforms contributes naive vernacular geographic knowledge, embedded within rich semantic detail, far beyond the level captured by point coordinate geotags. Understanding how we can capture this knowledge and use it to generate insights into the geographic concept of place opens up a substantial volume of data to be considered for research, which is directly embedded with rich place-based geographic knowledge.

New developments in natural language processing (NLP) have generated _large language models_ (LLMs), which have enabled much deeper semantic representations of text to be generated. Such models produce _embeddings_, which act as detailed numerical representations of text, capturing contextual semantic information, beyond the capabilities of former semantic models like Word2Vec [@mikolov2013]. This thesis extensively utilises LLMs to produce analysis of social media text that incorporates as much semantic information as is currently possible, improving both the accuracy of the identification of place names from text, to generate processed text data used throughout this thesis, and to capture deeper semantic associations between locational mentions in @sec-footprint.

Geographic information in text primarily exists as place names, and all real-world place names are theoretically attributable to a location in space, which may be formally or informally defined. For example, in an informal written conversation, someone suggesting they live in 'near Liverpool' has a specific view of where 'near' refers to [@stock2018]. Additional geographic information in text also appears through prepositions, where places may be geographically associated; for example, 'Liverpool is near Manchester' [@montello2003]. Informal written language contributes the subconscious information that individuals use to form their perception of geography, unrestricted by any formal geographic model of space. Geographic information may then be derived from this text which reflects a view of geography that is more strongly associated with the everyday experience of individuals, and captures a sense of place.

The prevalence of social media data for use in geographic research has generated a renewed interest in the concept of place [@wagner2020;@purves2019;@westerholt2018a], as contributions to social media are theorised to capture informal knowledge that represents a place-based understanding of geography [@goodchild2011;@sui2011]. In the context of language, this place-based knowledge is generated through 'vernacular geography', which describes the natural language used when informally describing geographic locations [@gao2017a;@goodchild2011;@waters2003;@hollenstein2008]. This informal knowledge incorporates biases regarding locations, better representing human perceptions of geography, compared with formal administrative definitions. While many works have considered the formalisation of place through geotagged social media posts, few have considered how associated text may directly generate insights into the vernacular geography of individuals.

In contrast to geotagged locations on social media, place names mentioned directly in social media text arguably more appropriately capture the informal nature of place. Embedded within a broad semantic context, and forming informal associations built directly through vernacular geography, these place names contribute a wealth of detailed and informal geographic information, that isn't available through geotags alone. The volume of associated data is also much greater when text can be considered, as many Tweets do not contain geotags (~99% of Tweets) [@leetaru2013], and alternative social media websites do not always have geographic metadata.

### Thesis Scope

The overarching goal of this thesis is to expand research relating to the extraction of place-based knowledge from informal natural language text. To achieve this, embedded places names are extracted from a large corpus of comments from the social media website Reddit^[www.reddit.com], using a task and data specific geoparsing methodology. @sec-transformer first outlines limitations of existing geoparsing systems with respect to this goal, and demonstrates the ability to build task specific Named Entity Recognition (NER) models for place name identification, greatly improving geoparsing performance. In @sec-connections and @sec-footprint this methodology is then implemented, to extract geographic locations from these informal comments, methods are outlined for harvesting the place-based knowledge that accompanies these locational mentions.

@sec-theory first outlines the geographic concept of place, how natural language is inherently connected with this concept, and highlights existing formalisations of place. @sec-quant then outlines how geotagged social media data have been used to quantify place, before demonstrating that non-geotagged social media may provide an even richer depth of place-based geographic knowledge through associated natural language text. @sec-extract then details methodologies that enable geographic information to be extracted from text, and how these methods may be improved for the goals of this thesis. @sec-litcon then concludes this chapter.

## Natural Language and Place Theory {#sec-theory}

The majority of geographical research considers analysis from the perspective of space, where locational boundaries are represented by administrative definitions and strict coordinate systems. While clearly defined geographical units allow for easily quantifiable geographical analysis, it rarely conforms with how people informally understand their own geography. Instead, the geographic concept 'place' considers how people perceive and interact with their geographic environment, encompassing personal experiences of the physical characteristics of locations, as well as the social and personal connections formed with them [@canter1977;@cresswell2014]. To build an understanding of a place, people generate cognitive representations of their known geographic environment, capturing vague but useful spatial information [@smirnov2016;@lukermann1961], which is shared through informal communication with others [@hollenstein2008].

### Formalisations of Place

Many works conceptualise this informal understanding of geography through 'mental maps'. These typically refer to the cognitive visualisation of a geographic environment, representing collective and experiential geographic knowledge, relating to both places, and the relationships between them [@kaplan1976;@gould1968;@pocock1976]. Mental maps exhibit a variety of biases, for example, they are more detailed with respect to locations that we are familiar with, while others may be less detailed or even absent entirely [@gould1986]. The scale and distance between features in mental maps are warped [@peake2004]; prominent roads may appear larger than in reality, skyline features in a city may be perceived as less prominent due to their irrelevance to an individual at street level [@gould1986;@lynch1964], and good transport connections narrow the time it takes to reach connected locations, which in turn reduces the perceived distance between them [@massey2008]. Intermediate features along common routes also have varying levels of importance to individuals; unimportant features may appear less prominent than in reality or absent altogether [@carr1969;@kaplan1976].

The characterisation of mental maps has been well studied from a qualitative perspective, often featuring individual participation for the physical construction of hand-drawn sketches [@lynch1964;@pocock1976;@montello2003a;@haney1978;@canter1977;@murray1979;@lee1973;@gould1986;@goodey1974]. Such approaches typically consider more localised areas that are familiar to selected participants [@pocock1976;@haney1978;@canter1977], for example, mapping landmarks and regions within cities. Others have considered the broad characterisation of larger regions like entire countries [@gould1968;@goodey1974], where mental maps are less detailed, instead contributing generalised information regarding areas that are deemed important to the participant. Inherently, these techniques capture subjective information from individuals, which may not necessarily conform with a general collective knowledge of these geographic locations.

Informal communication contributes the information used to build these mental maps through 'vernacular geography' [@egenhofer1995], where places are described using imprecise referents, non-administrative names, and an understanding of footprints that do not conform with the formal administrative boundaries given to them [@gao2017a;@goodchild2011]. There is therefore a clear link between informal written communication relating to geographic locations and the knowledge used to form the mental maps that encapsulate place-based geographic knowledge.

## Quantifying Place {#sec-quant}

In attempts to quantify place, many works consider the ability to capture the informal footprints of landmarks that appear on mental maps, which are often named 'cognitive regions' [@montello2014]. Like all components of mental maps, these cognitive regions form through experience and interaction with geographic environments [@montello2014]. These vague regions vary between populations and individuals, generating perceptions of regions that do not agree with formal administrative definitions, or even intrinsic spatial concepts. For example, San Diego is perceived to be less part of Southern California than Los Angeles, despite being much further south, [@montello2014;@gao2017a].

Early attempts to quantify these vague cognitive regions utilised participatory mapping exercises. For example, members of the public have been asked to delineate the area of a city they consider to be 'downtown' [@montello2003a]. Other works have considered the ability to use place names themselves to use topographic features to delineate place boundaries [@fisher2004], or use web harvesting queries [@pasley2007]. For example, the search engine query 'Midwest cities such as' returns a list of cities perceived to be within the vague Midwest region of the United States [@arampatzis2006]. In an early approach using unstructured text, the 'Southwest' in America was delineated using descriptions in publications, which were manually interpreted to identify locations internal or external to this vague region [@byrkit1992].

While early approaches considered individual participation through mapping exercises to generate cognitive regions and mental maps, alternative forms of data have generated a renewed interest in the study of place. Large volumes of _easily accessible_^[During the write-up of this thesis, the Twitter API was restructured to [remove all free academic access](https://x.com/XDevelopers/status/1621026986784337922?s=20). This change prompted a similar decision to [substantially restrict the free Reddit API](https://www.redditinc.com/blog/apifacts). As a result, it is now debatable how easily accessible these data are. Despite this, this thesis is written with this footnote as a caveat, and future mentions will treat these data as open.] social media data are now available that have strong associations with informal knowledge that reflects a place-based understanding of geography [@goodchild2011]. In a systematic literature review of place-based GIScience research, @wagner2020 note that place-based research is indeed increasing in popularity, which conforms with observations in other works [@purves2019;@westerholt2018a]. Results show that the majority of studies use online geotagged social media data, and unlike in early research, qualitative mapping exercises are less common.

### Capturing Place-based Knowledge though Alternative Forms of Data

With user generated, geographically referenced content through social media, mapping platforms and Wikipedia there is now a wealth of information that @goodchild2007 terms 'Volunteered Geographic Information' (VGI). These data sources present a large collection of continually updated, informal references to places, providing insight into the vernacular geography and mental maps that derive from the sense of place. While traditional formalisations of space can be considered 'top-down', as boundaries and names generated primarily by government organisations [@agnew2005], formalising place is more appropriate through a 'bottom-up' approach using VGI, built from the collective knowledge that individuals contribute on these social media whose geographic knowledge is developed through experience [@sui2011]. Unlike past work that considered active survey and participatory mapping exercises with limited participants to extract informal geographic knowledge, this data enables mental maps and vernacular geography to be studied in a much larger and comprehensive volume.

These data sources come in broadly two forms:

1. Spatially explicit; [GeoNames](https://geonames.org), and collaborative mapping platforms [OpenStreetMap](https://openstreetmap.org) (OSM) and [Wikimapia](https://wikimapia.org) which embed the locational information as an attribute alongside other information.

2. Spatially implicit; geotagged Tweets posted on [Twitter](https://twitter.com), or photographs from [Flickr](https://flickr.com), where the locational information is secondary to the communication between users [@antoniou2010].

Collaborative mapping platforms like OSM and Wikimapia allow users to directly input geographic coordinates and place names onto a base map. In these cases, volunteers contribute geographic features, which may include vernacular geography as colloquial names through polygon boundaries and point locations [@leidner2011;@gao2017b;@sui2011]. OSM for example contains neighbourhood names in London contributed by users, many of which are not formal administrative names [@clasper2018]. OSM however, is curated in a way that closely follows a spatial view of mapping, focussing on vector information and single points for named places. Wikimapia differs from this, in that the place information is derived mainly from user contributed bounding boxes, includes descriptive information, images, and notably encourages a more place-based view of mapping. For this reason @ballatore2019 suggest that Wikimapia appears more suitable for information retrieval and search tasks where natural language is used, despite its decline in popularity and limited mention in research. However, unlike OSM, Wikimapia data is only accessible through a limited API. In a sense, these mapping platforms act as informal gazetteers, providing user contributed geographic information that isn't collated from administrative datasets.

Alternatively, spatially implicit data sources can be seen to contribute place-based locations rather than space-based, as results derived from them represent the cognitive perceptions of place which do not necessarily correlate with any formal definitions [@quesnot2015;@sui2011;@sui2009;@hollenstein2010;@cresswell2014;@wolter2018;@goodchild2011a;@purves2011]^[It is worth noting that the concept of VGI may not directly align with spatially implicit sources, given they are not 'volunteered' [@purves2011]. In this thesis however, I do consider spatially implicit sources of user-generated content as a form of VGI, given this is a common observation in literature, and allows for a distinction between normal user-generated content, and those that contain a geographic element. Arguably, the content itself is volunteered, and geographic elements like geotags are often opt-in features of these social media.]. A multitude of works have built on this ability to conceptualise place using implicit VGI [@wagner2020], augmenting traditional gazetteers with place-based information [@gao2017;@gao2013;@quesnot2015;@goodchild2011], generating vague place-based footprints relating to landmarks and locations [@mckenzie2017;@gao2017a;@jang2019;@twaroch2019;@westerholt2018], or examining informal perspectives regarding locations [@solymosi2021]. Notably, the results derived from the generation of vague footprints appeared to largely agree with mapping exercises and questionnaires [@twaroch2019;@gao2017a].

The geographic component in these works almost always comes from 'geotags', where geographic metadata as a single coordinate pair is associated with a single social media post. This reliance on geotags limits the data available, on Twitter for example, they are only present in around 1-2% of Tweets [@leetaru2013]. Additionally, while specific coordinate information may be obtained from geotags with an apparent degree of accuracy, coordinates often do not truly reflect the vague nature of the content they are associated with. A person may perceive themselves as being at a particular location that they have cognitively defined, but when geotagging their location, the tag resolves to the formal administrative location [@quesnot2015]. A similar issue was observed by @kropczynski2018 who note that Tweets that describe an incident in emergency situations frequently have a geotag that does not reflect the location being described. This reflects a broader problem with geotags; the content of Tweets does not necessarily relate to the geotagged location.

There are also a multitude of demographic biases to consider with social media platforms and user-generated content. Contributions to Wikipedia and Tweeting frequency are both influenced by demographic factors [@hecht2014;@ballatore2018;@ballatore2020], population density [@mullen2015;@graham2015;@ballatore2020], and urban areas have better coverage than rural ones [@hecht2014;@ballatore2020]. Similar biases also appear in OSM and the GeoNames gazetteer [@graham2015;@jackson2013]. At a national level, these issues are likely attributable to the ad-hoc nature of VGI; volunteers contribute what they want, favouring well known and high population areas, without consideration of scientific sampling methods [@zhang2018]. Collaborative mapping platforms also suffer from few users contributing the majority of additions [@ballatore2019], meaning that any research derived from them does not necessarily capture the perspective of a population. Text and image based (spatially-implicit) VGI like Twitter and Flickr do however suffer higher quality concerns in comparison with collaborative mapping platforms, in terms of the reliability of the data [@senaratne2017], but with an overall much larger volume of contributions.

### Text-based Volunteered Geographic Information

While much of this current place-based research focusses on select geotagged social media, and examines place through the generation of vague boundaries relating to vernacular place names, place may be conceptualised in many ways. This thesis instead considers how language itself can be examined to distil and quantify place-based knowledge, voluntarily expressed through informal text as direct communication between social media users. Potential formalisations of place generated from text move beyond just the generation of feature footprints within mental maps, opening up the opportunity to conceptualise and quantify alternative perceptions of geography, built directly from vernacular knowledge derived from these informal textual communications.

Past work that has considered the geographic properties of text is limited, however an early example delineated the American 'Southwest', a vague vernacular place name, by identifying mentions in natural language text descriptions of associated locations [@byrkit1992]. Place names have been automatically extracted from housing advertisements, identifying unknown informal place names, with vague boundaries delineated using co-occurring names [@hu2019]. Hiking descriptions also provide frequently occurring place names that are fine-grained and often absent from existing gazetteers [@moncla2014;@palacio2015], while text posts on Twitter and FourSquare contribute fine-grained place names [@li2014;@han2018], which are useful during emergency events [@grace2020]. Automatically extracting place names from text-based VGI removes the requirement of empirical data collection regarding fine-grained or informal place names, which is both time-consuming and difficult to scale [@vasardani2013a]. Additionally, while geotags do provide quantifiable geographic information that may be used to derive vague cognitive footprints, they are limited, compared to the depth of information theoretically contributed through unstructured semantic knowledge in text.

Twitter is by far the most popular social media website used in research, due to the accessible API, large data volume, and geotagged posts. The use of geotagged Tweets for text-focussed place-based research however presents several issues:

1. **Broad Topics;** even when Tweets are geotagged, their content is may be unrelated to the location geotagged.
2. **Limited context;** Tweets are very restrictive in character count, meaning their context is limited.
3. **Informal language;** While all language on social media is more informal compared with alternative text sources like news articles, the language on Twitter more frequently contains abbreviations and misspellings, particularly due to the limited number of characters.

Given the content of Tweets is often unrelated to the geotagged location, any semantic information extracted from Tweets cannot be directly associated with geography. As a result of this, many works that have considered geographic variations within the textual content of Tweets to be attributable to dialects [@arthur2019;@doyle2014;@goncalves2014;@russ2012;@eisenstein2014;@huang2016;@han2012;@zheng2018]. This works under the assumption that Tweet authors are residents at the geotagged location.

With the ability to accurately geoparse locations from text, locations can be extracted that are embedded directly within a related semantic context. Instead of just dialects, this context presents a broad range of topics relating to locations, built from the mental associations of contributing authors, incorporating cultural information, and capturing cognitive biases. Importantly, this also means that any form of text may contain a geographic component, shifting the reliance away from solely using geotagged social media data.

Unlike the microblogging website Twitter, [Reddit](https://reddit.com) is a forum-based social media platform, encouraging open discussion regarding a range of topics in dedicated sub-forums called subreddits. Subreddits are created and moderated by volunteers, meaning they cover a multitude and range of subjects. The use of Reddit comments for text-focussed place-based research comes with several benefits over alternative social media:

1. **Subreddits;** Reddit is split into individual 'subreddits' each with a specific topic of discussion, with many relating to locations across the UK. This limits the volume of noise in comments, as relevant subreddits can be targeted.
2. **Context;** Unlike Twitter, Reddit comments are much less restricted in character length, meaning they contain a greater semantic context, and are less likely to be filled with abbreviations.
3. **API;** There was a brief period following the Twitter API changes that Reddit still gave unrestricted access to data archives. This has since changed, but Reddit still provides are more open API compared to the current Twitter API terms.

The use of subreddits is a key driver in the ability to exclude noise from Reddit, that isn't possible through alternative social media like Twitter. For example, the [United Kingdom subreddit](https://reddit.com/r/unitedkingdom) includes a list of smaller local British subreddits, ranging in scale from country level (`/r/England`), regional (`/r/thenorth`, `/r/Teeside`), to cities (`/r/Manchester`) and small towns (`/r/Alnwick`). In total there are 213 subreddits that relate to 'places' within the United Kingdom^[https://www.reddit.com/r/unitedkingdom/wiki/british_subreddits]. By selecting these subreddits, all comments extracted relate to topics of discussion surrounding a range of locations across the UK, rather than general topics of discussion. A full archive of all comments from each subreddit is kept by the [Pushshift](https://pushshift.io/) Reddit archive [@baumgartner2020]^[Due to the Reddit API changes, this archive is now only available to Reddit moderators.].

Reddit however is underused in previous geographic research, primarily due to the complete lack of any accessible geographic component like geotags. This limitation was overcome in this thesis, by identifying place names and attributing coordinate information through a task-specific geoparsing methodology. The following section outlines geoparsing, and notes on the ability to build a model that targets the task of geoparsing for place-based research in social media.

## Extracting Geographic Information from Text {#sec-extract}

The primary source of extractable geographic information in text exists as place names embedded within their surrounding semantic context. The primary method for associating geographic information with place names in unstructured text is known as 'geoparsing'. In geoparsing systems, place names are first automatically identified in text, in a process called _toponym detection_, or _place name identification_ [@leidner2011;@stokes2008], then associated with geographic coordinates, in a stage generally known either as _geocoding_, _toponym disambiguation_, or _toponym resolution_ [@gritta2019;@leidner2008;@buscaldi2008;@buscaldi2008a;@buscaldi2011]. In a typical geoparsing system, a Named Entity Recognition (NER) model first identifies any place names in text, which are then resolved to their most likely coordinate using the document context, and a gazetteer [@purves2018]. @fig-geoparsing gives an overview of this process.

![Overview of an example geoparsing methodology.](./02_figures/geoparsing.pdf){#fig-geoparsing}

The terminology with respect to geoparsing systems often differs between different disciplines, for example geocoding in Geographic Information Retrieval (GIT) research is confusingly known as 'geoparsing' [@purves2018;@moncla2014]. In this chapter I have therefore made the decision to avoid specialised terminology and instead use the following definitions. Geoparsing systems are split into two stages;

1. **Place name identification;** A technique is used to identify unique spans of text that contain a place name.
2. **Geocoding;** Place names are associated with coordinate information.

Once associated with coordinates, these are no longer solely place names, but 'locations' with an associated place name. I therefore avoid the usage of 'toponyms', which is a synonym of 'place names'.

While geoparsing is widely used, especially in the context of querying and GIT research, there are key limitations of these systems when considering a focus on place-based knowledge extraction for this thesis. These issues may be broadly categorised into improvements to the two stages of geoparsing systems;

1. _Improvements to **place name identification**_
    * **Task specific;** Models should be trained using an annotation scheme that properly considers place names.
    * **Data specific;** Model should be trained on data relating to the specific task.
    * **Recognise non-locations in context;** Metonyms for example are common in social media text, e.g. Sporting teams named after cities, and may be identified by more complex models.
2. _Improvements to **geocoding**_
    * **Fine-grained locations;** Gazetteers exclude many fine-grained locations, and geoparsing systems usually only consider place names down to city level.
    * **Data representations;** Gazetteers only represent locations as single coordinate pairs, regardless of scale.
    * **Relationships;** Locations are not considered in relation to each other, only as individual entities. Incorporating relationships between locations would assist in geocoding vague locations.

The following section first outlines how these two key stages are currently handled in existing geoparsing systems. The issues relating to these existing solutions are then outlined with respect to the goals of this thesis; extracting geographic place-based knowledge from unstructured social media text. Recent developments in natural language processing are then highlighted, enabling both more accurate place name identification systems, and deeper semantic information to be extracted from text.

### Place Name Identification

Place name identification techniques in geoparsers may be broadly allocated into two categories, gazetteer-based, and machine learning based, often utilising NER [@purves2018]. A simple system for the identification of place names using a gazetteer will simply look up each word within a document, and see if it matches an entry within a gazetteer. This method has low recall and accuracy, but is relatively fast, and recent implementations usually focus on live-streamed Twitter data to quickly identify locations being tweeted about in ongoing emergency situations [@itoh2016;@middleton2014]. Importantly, these locations have a limited spatial extent, so the number of queried locations can be limited, reducing the number of false-positives. Other systems may even forgo the use of a gazetteer and simply rely on the identification of _noun phrases_, assuming that any location would be expressed through nouns [@malmasi2016].

While gazetteer and rule-based approaches provide a simple method for the identification of place names, they suffer from two key issues. First, gazetteers may not be complete for all regions; even the widely used _GeoNames_ gazetteer has incomplete information for the United Kingdom [@stock2013]. Second, words identified as a location in text may not be necessarily a referent to a location (For example 'Reading' or 'Bath' in the UK; See @fig-lookup), broadly defined as _place name ambiguity_ [@wacholder1997;@gritta2019;@brunner2008].

![Place name identification by simple lookup [@bird2009]](./02_figures/locations.png){#fig-lookup}

@wacholder1997 identify two types of place name ambiguity; _structural ambiguity_, in which the structure of the words constituting a place name are ambiguous (e.g. _'North Yorkshire'_; is _'North'_ part of the name?), solved in part with noun phrase extraction [@lieberman2011;@malmasi2016]. The second being _semantic ambiguity_ in which the type of entity being referred to is ambiguous (_'Paris'_ the city, or _'Paris Hilton'_). For this reason, most modern geoparsing systems make use of NER for place name identification, which are typically able to infer from context whether phrases in text refer to place names.

A further consideration when dealing specifically with place name ambiguity is _metonymy_; defined as a figure of speech in which the speaker uses _'one entity to refer to another that is related to it'_ [@lakoff1980]. In terms of geoparsing, a location based metonym does not refer to a location, but to a different but related entity [@leveling2008]. Common geographic metonyms appear with language use in relation to political events, for example in the sentence _'**London** voted against Brexit'_, the place name London refers to the people of London. @gritta2017 note on the pitfall of misclassifying the sentence _'**Moscow** talks to **Beijing**'_ as such a sentence does not inform the reader _where_ talks are occurring. Use may also include elliptical constructions, _'**Prescot** and **Liverpool road**'_ as both are street names, Liverpool road, and Prescot road [@leveling2008].

The reported performance of recent NER models is high, with current state-of-the-art models achieving an F~1~ scores above 0.93 on the 'CoNLL 2003 NER Task' dataset [@tjongkimsang2003;@baevski2019]^[See https://nlpprogress.com], approaching human levels of accuracy. As such, the place name identification stage of many geoparsing systems generally focus on using NER models (See Table \ref{tbl-geoparsers}), despite these models being built around the general identification of entities, rather than solely place names. @gritta2019 suggest that this leads to most geoparsing systems ignoring issues like metonymy altogether, as it is not considered when annotating the corpora used to train these models.

Despite the primary focus of geoparsing systems being the single goal of identifying and geographically grounding place names, they are crucially only able to consider _entities_ in the sense of the labelled corpora their NER models were trained on. Common NER training corpora typically focus on either the [CoNLL 2003](<https://www.clips.uantwerpen.be/conll2003/ner>), or [OntoNotes](<https://catalog.ldc.upenn.edu/LDC2013T19>) specifications. Such schemes do not specifically annotate place names, but consider a variety of related and unrelated entities.

For example, models trained using the OntoNotes 5 corpus support the following entities that relate to place names:

* **FAC:** Buildings, airports, highways, bridges, etc.
* **ORG:** Companies, agencies, institutions, etc.
* **GPE:** Countries, cities, states.
* **LOC:** Non-GPE locations, mountain ranges, bodies of water.

Additionally, they support the following expressions that relate to geographic information:

* **QUANTITY:** Measurements, as weight or distance.
* **ORDINAL:** 'first', 'second', etc.
* **CARDINAL:** Numerals that do not fall under another type.

While models trained on the CoNLL 2003 corpus support the following entities that relate to place names:

* **LOC:** Name of politically or geographically defined location (cities, provinces, countries, international regions, bodies of water, mountains).
* **ORG:** Named corporate, governmental, or other organizational entity.

Note that **ORG** in both cases only refers to a geographic entity when is it associated with a building or site of a named organisation, for example schools or chain supermarkets. This tag would apply to both bold entities in the following sentences, despite only the second referring to a specific geographic location:

a. 'The new {**Apple**}~ORG~ iPhone.'

b. 'The {**Tesco**}~ORG~ is at the end of the road.'

It also makes little sense from a geographic point of view to split certain entities into separate categories. **FAC**, or **ORG** will often both refer to buildings, while **GPE** and **LOC** are both named locations, and collectively they should all be considered place names which are resolvable using a gazetteer.

### Geocoding

While locational entities in text were among the first to be identified [@piskorski2013], the need to resolve them was not considered until much later [@leidner2008]. In general, geocoding approaches consider a place name $t$ with $n$ possible referents, within a document $d$, and using context place names $c_0,\dots, c_k$ within the document $d$, $t$ may be resolved to a single referent [@buscaldi2011]. In this case, geocoding may be considered a specific implementation of the general task of 'Word Sense Disambiguation' [@buscaldi2008]. The referent place names are scored based on their likelihood to be correct, and the highest score selected. Methods for correctly geocoding an ambiguous place name may be split broadly into either rule-based or machine-learning based [@buscaldi2008;@gritta2018].

Of rule-based methods, the most simplistic are those which consider the coordinate position of all place names (identified and ambiguous) on a map. For example @smith2001 represent all possible locations for all identified place names within a document on a map, weighted by the number of times they appeared, removing those that appeared significantly outside the centroid of the map. Alternatively, if the source of a certain document is known, it is likely that place names identified as being significantly outside the source location may be incorrect [@buscaldi2010]. Meta information may also be used to remove less likely place names, such as those with a low population [@gelernter2013]. Some methods use contextual words in a document, for example _demonyms_ (referring to residents of a place, e.g. Londoners) [@lieberman2011;@zhang2014], Other work has considered _location indicative words_, where unique words were automatically extracted from tweets with known locations, and used to geolocate text without requiring specific mentions of the location [@han2014;@chi2016;@han2012]. @chi2016 for example are able to associate the tweet _'I plan to take a tram to the federation square this arvo to watch the cricket...'_ with the Australian city Melbourne due to the location indicative words 'tram', 'federation square', and 'arvo'.

Some modern solutions rely on machine learning techniques to resolve place names to their most likely coordinates. The geoparser _Mordecai_ for example uses [Keras](https://keras.io) implemented neural networks trained on annotated data to identify correct gazetteer entries for place names [@halterman2017]. However, as outlined on Table \ref{tbl-geoparsers}, this is the only geocoding system that uses a machine learning methodology, while heuristic rules are typically implemented.

\begin{table}
    \caption{\label{tbl-geoparsers} Comparison between open source geoparsers}
    \centering
    \fontsize{9}{11}\selectfont
    \begin{tabular}[h]{llll}
    \toprule
        \textbf{Geoparser} & \textbf{Detection} & \textbf{Resolution} & \textbf{Gazetteer} \\
    \midrule
        Edinburgh Geoparser & Rule-based & Rule-based & GeoCrossWalk/GeoNames \\
        GeoTxt & NER & Rule-based & GeoNames \\
        geotext & Rule-based & None & GeoNames \\
        geograpy & NER & Rule-based & pycountry/GeoLite2 \\
        Mordecai & NER & ML & GeoNames \\
        CLIFF-CALVIN & NER & Rule-based & Reuters \\
        geography3 & NER & Rule-based & Wikidata \\
    \bottomrule
    \end{tabular}
\end{table}

### Geoparsing for Place-based Research

The concept of a place name as an entity defined by annotated NER corpora hinders the approach that geoparsing work takes, considering only (and any) defined place name as a geographic location within text [@gritta2017]. The popular Python geoparser Mordecai, for example, uses an NER tagger provided through the SpaCy Python library [@honnibal2017], considering **GPE** (Geopolitical entities), **LOC** (Locations), and **FAC** (Facilities), which do not necessarily relate to geographic locations in certain contexts. Due to the reliance on large labelled corpora for NER training, and limited source of geography specific data [@karimzadeh2019;@stock2013;@gritta2019], little work has considered building NER models from the group up to specifically focus on the task of place name identification.

Existing annotated NER corpora are also typically built using American news articles [@tjongkimsang2003], meaning their performance on other text is limited. Social media text in particular is unique in comparison with more formally written and structured text, given documents themselves (e.g. Tweets) are typically short, context is limited, and abbreviations and vernacular language is common [@gelernter2013]. Additionally, capitalisation of proper nouns is not always used, which can make it difficult for existing models accurately parse place names.

Data annotation itself is not a simple task, requiring domain knowledge to ensure high accuracy [@petrillo2010], and being time-consuming to produce [@middleton2016;@wallgrun2018;@gey2006]. The lack of large, high quality, labelled geographic natural language data is well noted by many authors in this subject area [@tobin2010;@speriosu2013;@weissenbacher2019;@weissenbacher2015;@gritta2018;@karimzadeh2019], which likely contributes to the lack of task-specific models in geoparsing systems. Given context is limited and capitalisation is often missed in social media text, annotation can be difficult. For example, _'I love reading.'_ is impossible to correctly annotate without considering broader context or associated metadata. If this comment was taken from an online forum dedicated to the discussion of Reading in southern England, then an annotator may make an educated decision to annotation this as a place name. However, if the forum is instead primarily associated with the discussion of Edinburgh in Scotland, it is far more likely to be a noun, not requiring annotation. These problems are more prevalent on Twitter, but are solved in part when using Reddit comments, given the language is typically more structured, context length is far greater, and comments are organised into sub-forums.

While existing NER annotation schemes are not optimised for place name identification, some work has considered the need to identify _geographic_ or _spatial_ entities. The 'SemEval-2019 Task 12: Toponym Resolution in Scientific Papers' considered an annotation scheme for building NER models that only focussed on place name identification [@weissenbacher2019]. Alternatively, SpatialML is a natural language annotation scheme that presents the **PLACE** tag for any mention of a location, also including and labelling nominal (non-specific) mentions of places [@mani2010]. Tasks identified by the [Semantic Evaluation Workshop](https://semeval.github.io/) expanded this annotation scheme to include other entities that relate to spatial language [SemEval-2015 Task 8: SpaceEval, @pustejovsky2015]. The annotation scheme for this task is described by the ISO-Space specification [@pustejovsky2017]:

* **PLACE:** Geographic entities including lakes and mountains, administrative entities like towns or counties, and general locations like buildings.
* **PATH:** Locations with the potential for traversal or boundaries, like roads, rivers, or coastlines.
* **SPATIAL_ENTITY:** Spatially relevant objects that do not fit into the above tags. In the context of this thesis this primarily relates to the person describing their location:
  * '{**I**}~SE~ am next to a building.'

These entities also contain some metadata, particularly the `form` tag distinguishes whether the tagged text relates to a nominal form, or a proper name:

a. 'I camped next to the municipal {**building**}~PL(`form=NOM`)~'

b. 'I travelled north to northern {**Italy**}~PL(`form=NAME`)~'

In addition to these geographic entities, the **MEASURE** tag provides modifying information that relates to geographic entities.

a. 'I am {**300 meters**}~MEASURE~ south of a church.'

b. 'I am {**10 minutes**}~MEASURE~ from the coast.'

Similar works have considered the requirement for geography-specific corpora for training geoparsers, aiming to include a high proportion of geographic expressions and place names [@stock2013;@wallgrun2018;@wallgrun2014]. One corpus annotates place names identified in a collection of Tweets [@wallgrun2018], intended for the training and evaluation of a short-context social media geoparsing system. Another presents a training dataset that considers metonyms in NER [@gritta2017], demonstrating that a deep learning model is capable to handling metonyms with improved accuracy compared with traditional heuristic approaches [@lieberman2011;@leveling2008]

Finally, geoparsers are typically intended as a general system for attributing locational information from documents. Due to this, the scope of geocoding is usually global, which causes many ambiguous place names to appear (e.g. _Cambridge, UK_ vs _Cambridge, US_). The text being considered in these systems often contains a significant context that is not geographically focussed, and as such does not provide any context to assist with geocoding. In this case, ambiguous entities appear in cases where the NER classifier incorrectly classifies other entities as geographic, such as people.

In this thesis, I instead consider that a geoparsing system may be built to specifically target locations within the United Kingdom, reducing the number of ambiguous place names that may occur, and allowing for fine-grained locations to be geoparsed. By specifically targeting subreddits that focus on local place discussions, the Reddit comments geoparsed in @sec-connections and @sec-footprint incorporate larger proportion of place names and relevant context, compared with general text.

### Developments in Natural Language Processing

The exponential increase in computational power has brought with it the ability to process large quantities of unlabelled text. This development has recently led to the creation of general purpose ‘large language models’ (LLMs) that implement the 'transformer' architecture, and use semi-supervised learning to train using very large corpora [@vaswani2017]. For example, Google’s pioneering BERT model was trained using the entirety of Wikipedia, and over 11,000 books [@devlin2019].

The architecture specific to these models has enabled 'transfer' learning, by which a pre-trained model like BERT is used as a base, and fine-tuned to be task specific. Fine-tuning these pre-trained models in NLP has overtaken the popularity of manually trained word embeddings (Word2Vec; @mikolov2013), which are limited by the volume of data provided to them, alternative pre-trained embeddings like ELMo [@peters2018], as the transformer architecture provides improvements over the traditionally used Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN). Unlike previous models, transformers are more easily fine-tuned on new data, requiring only a single additional output layer to achieve task specific results.

Much recent attention has been given to language models, primarily through the development of ChatGPT^[https://openai.com/blog/chatgpt], which utilises a much larger language model compared with BERT, known as a generative pre-trained transformer (GPT), which focuses on text generation, rather than traditional NLP tasks like token classification for NER.

## Conclusion {#sec-litcon}

The primary goal of this thesis is to expand the knowledge relating to place in geography. This chapter has outlined limitations in past work that considered the use of alternative forms of data through social media for this task, where the reliance on explicit geographic markers on these data has limited the depth of knowledge accessible. This reliance on geotagged social media has resulted in a primary focus on the generation of cognitive regions, which only presents one potential aspect of the place-based knowledge available. Instead, I consider that additional informal geographic knowledge is embedded within informal communications between social media users, accessible directly through associated social media text. Unlike geotags however, geographic information in text is not directly quantifiable, relying on a method of extraction.

The extraction of geographic information from text primarily focusses around the task of geoparsing, where place names in text are attributed to geographic coordinate information. Existing work in this area has established that the best place name identification performance typically results from the use of pre-trained NER models, which are able to consider semantic context to accurately identify place names. These models however do not solely target place names in their annotation schemes, and are not usually trained on social media data. Attempts to generate geography specific annotated corpora for such models are limited, and not specific to the corpus primarily used in this thesis. Additionally, when geocoding identified place names, existing geoparsing models typically consider a global geographic scope, meaning specific countries like the United Kingdom are unable to be targeted. This therefore results in a large volume of unrelated place names within associated gazetteers, increasing the potential for noise in any outputs. Global gazetteers like GeoNames are also more limited when considering fine-grained locations like street names, which are an important component in place-based geographic knowledge.

In this thesis I therefore construct a task and data specific geoparsing methodology, building an NER model for place name identification, trained specifically to identify place names from UK specific Reddit comments. The geocoding stage of this system similarly only considers a UK specific corpus, using the OS Open Names gazetteer, which includes fine-grained locations, but excludes locations external to the UK. With this methodology established, I accurately extract all UK specific geographic locations embedded within these Reddit comments, resulting in a large volume of informal geographic references to locations associated with coordinate information, allowing for place-based geographic analysis.

Instead of focussing on the generation of cognitive regions, this thesis instead demonstrates how text itself may be utilised to quantify alternative aspects of place-based knowledge. In @sec-connections I first consider that while the strength of connection between geographic locations may be established through physical geographic movements and migration of populations, there exists an implicit knowledge of subconscious associations between locations, built on informal geographic knowledge. Associated text is used directly to establish these associations, taking inspiration from research that has established that co-occurring nouns typically share an implicit semantic association. In @sec-footprint I consider directly how embedded semantic information relating to locations varies geographically, generating semantic footprints through the use of a large language model, which embeds with geographic knowledge of the Reddit users in this corpus.
